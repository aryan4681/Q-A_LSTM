{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyzCpNcYtbzD",
        "outputId": "f2de5ffb-455d-4dc8-8427-a1cdec263f7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-07-28 07:09:02--  https://qnamakerstore.blob.core.windows.net/qnamakerdata/editorial/english/qna_chitchat_professional.tsv\n",
            "Resolving qnamakerstore.blob.core.windows.net (qnamakerstore.blob.core.windows.net)... 20.60.81.165\n",
            "Connecting to qnamakerstore.blob.core.windows.net (qnamakerstore.blob.core.windows.net)|20.60.81.165|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 981694 (959K) [text/plain]\n",
            "Saving to: ‘qna_chitchat_professional.tsv’\n",
            "\n",
            "\r          qna_chitc   0%[                    ]       0  --.-KB/s               \rqna_chitchat_profes 100%[===================>] 958.69K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-07-28 07:09:03 (7.11 MB/s) - ‘qna_chitchat_professional.tsv’ saved [981694/981694]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://qnamakerstore.blob.core.windows.net/qnamakerdata/editorial/english/qna_chitchat_professional.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VByO9qANtdjz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.read_csv(\"qna_chitchat_professional.tsv\", sep=\"\\t\")[[\"Question\", \"Answer\"]].to_csv(\"qna_chitchat_professional.tsv\", index=False, sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiWU2yXqQpv9"
      },
      "source": [
        "# 1. Tokenisation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdaILR3kQxgJ",
        "outputId": "f731598d-27a6-43d4-b027-e52a267b5a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "pip install sentencepiece torch pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUvDSqdWTM9c",
        "outputId": "3eeb5670-6047-4848-8db2-18b50025bc09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded sample question: [15, 4, 42, 9, 65, 531, 14, 15, 5]\n",
            "Encoded sample answer: [15, 4, 7, 20, 8, 12, 19, 16, 292, 6, 15, 5]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"qna_chitchat_professional.tsv\", sep=\"\\t\")\n",
        "questions = data['Question'].tolist()\n",
        "answers = data['Answer'].tolist()\n",
        "\n",
        "# Train SentencePiece model\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    '--input=qna_chitchat_professional.tsv --model_prefix=tokenizer --vocab_size=1000 --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 --user_defined_symbols=<sos>,<eos>'\n",
        ")\n",
        "\n",
        "# Load tokenizer model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"tokenizer.model\")\n",
        "\n",
        "\n",
        "def encode_sentences(sentences):\n",
        "    return [sp.encode_as_ids('<sos> ' + s + ' <eos>') for s in sentences]\n",
        "\n",
        "# Encode questions and answers\n",
        "encoded_questions = encode_sentences(questions)\n",
        "encoded_answers = encode_sentences(answers)\n",
        "\n",
        "\n",
        "print(\"Encoded sample question:\", encoded_questions[0])\n",
        "print(\"Encoded sample answer:\", encoded_answers[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfYlAPhHU0F7",
        "outputId": "db34943d-dea6-4b59-ab0a-3bb5e21b4983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('<pad>', 0), ('<unk>', 1), ('<s>', 2), ('</s>', 3), ('<sos>', 4), ('<eos>', 5), ('.', 6), ('▁I', 7), (\"'\", 8), ('▁you', 9), ('s', 10), ('▁to', 11), ('t', 12), ('m', 13), ('?', 14), ('▁', 15), ('▁a', 16), (',', 17), ('▁me', 18), ('▁have', 19)]\n"
          ]
        }
      ],
      "source": [
        "vocab_list = [(sp.id_to_piece(idx), idx) for idx in range(sp.GetPieceSize())]\n",
        "\n",
        "\n",
        "print(vocab_list[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T8NMlNYVGDE"
      },
      "source": [
        "# 2. Padding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kvK25OxJVJfx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "tensor_questions = [torch.tensor(seq) for seq in encoded_questions]\n",
        "tensor_answers = [torch.tensor(seq) for seq in encoded_answers]\n",
        "\n",
        "\n",
        "padded_questions = pad_sequence(tensor_questions, batch_first=True, padding_value=0)\n",
        "padded_answers = pad_sequence(tensor_answers, batch_first=True, padding_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34LDvxevWDHf",
        "outputId": "ffa7da82-9e45-4657-a697-d2eb1a34ff7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padded Questions Shape: torch.Size([9793, 28])\n",
            "Padded Answers Shape: torch.Size([9793, 48])\n",
            "Padded Questions: tensor([[15,  4, 42,  ...,  0,  0,  0],\n",
            "        [15,  4, 42,  ...,  0,  0,  0],\n",
            "        [15,  4, 42,  ...,  0,  0,  0],\n",
            "        ...,\n",
            "        [15,  4,  7,  ...,  0,  0,  0],\n",
            "        [15,  4, 15,  ...,  0,  0,  0],\n",
            "        [15,  4,  7,  ...,  0,  0,  0]])\n",
            "Padded Answers: tensor([[15,  4,  7,  ...,  0,  0,  0],\n",
            "        [15,  4,  7,  ...,  0,  0,  0],\n",
            "        [15,  4,  7,  ...,  0,  0,  0],\n",
            "        ...,\n",
            "        [15,  4,  7,  ...,  0,  0,  0],\n",
            "        [15,  4,  7,  ...,  0,  0,  0],\n",
            "        [15,  4,  7,  ...,  0,  0,  0]])\n"
          ]
        }
      ],
      "source": [
        "print(\"Padded Questions Shape:\", padded_questions.shape)\n",
        "print(\"Padded Answers Shape:\", padded_answers.shape)\n",
        "print(\"Padded Questions:\", padded_questions)\n",
        "print(\"Padded Answers:\", padded_answers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOljc4nFW6UN"
      },
      "source": [
        "# 3. Split\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUqIwIpXXBC1",
        "outputId": "deacf69e-d1f7-4e61-c611-6036dd5f56f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IETgMgjaXLbF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_questions, test_questions, train_answers, test_answers = train_test_split(\n",
        "    padded_questions, padded_answers, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4yXg_TBXYN5",
        "outputId": "0f4132d7-37bc-4f9a-cf8a-120192579453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Questions Shape: torch.Size([7834, 28])\n",
            "Test Questions Shape: torch.Size([1959, 28])\n",
            "Train Answers Shape: torch.Size([7834, 48])\n",
            "Test Answers Shape: torch.Size([1959, 48])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Questions Shape:\", train_questions.shape)\n",
        "print(\"Test Questions Shape:\", test_questions.shape)\n",
        "print(\"Train Answers Shape:\", train_answers.shape)\n",
        "print(\"Test Answers Shape:\", test_answers.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzx3hcDFX_bz"
      },
      "source": [
        "# 4. Simple LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KUD7RTAlIWSD"
      },
      "outputs": [],
      "source": [
        "\n",
        "vocab_dict = {idx: sp.id_to_piece(idx) for idx in range(sp.GetPieceSize())}\n",
        "\n",
        "def indices_to_words(indices):\n",
        "    return [vocab_dict.get(idx, '') for idx in indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkf7a-krK0kL",
        "outputId": "39199c30-35d0-4de5-d499-4b7e57b4961a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/5, Batch 0/221, Loss: 6.9031\n",
            "Epoch 1/5, Batch 10/221, Loss: 3.8519\n",
            "Epoch 1/5, Batch 20/221, Loss: 3.2981\n",
            "Epoch 1/5, Batch 30/221, Loss: 2.7097\n",
            "Epoch 1/5, Batch 40/221, Loss: 2.1282\n",
            "Epoch 1/5, Batch 50/221, Loss: 2.2021\n",
            "Epoch 1/5, Batch 60/221, Loss: 1.2657\n",
            "Epoch 1/5, Batch 70/221, Loss: 1.1626\n",
            "Epoch 1/5, Batch 80/221, Loss: 1.2538\n",
            "Epoch 1/5, Batch 90/221, Loss: 0.9466\n",
            "Epoch 1/5, Batch 100/221, Loss: 1.2076\n",
            "Epoch 1/5, Batch 110/221, Loss: 0.6332\n",
            "Epoch 1/5, Batch 120/221, Loss: 0.8653\n",
            "Epoch 1/5, Batch 130/221, Loss: 0.9481\n",
            "Epoch 1/5, Batch 140/221, Loss: 1.2941\n",
            "Epoch 1/5, Batch 150/221, Loss: 0.6580\n",
            "Epoch 1/5, Batch 160/221, Loss: 0.6082\n",
            "Epoch 1/5, Batch 170/221, Loss: 1.2794\n",
            "Epoch 1/5, Batch 180/221, Loss: 0.7210\n",
            "Epoch 1/5, Batch 190/221, Loss: 0.6184\n",
            "Epoch 1/5, Batch 200/221, Loss: 0.5545\n",
            "Epoch 1/5, Batch 210/221, Loss: 0.5873\n",
            "Epoch 1/5, Batch 220/221, Loss: 0.4359\n",
            "Epoch 1/5, Average Loss: 1.4733\n",
            "Epoch 2/5, Batch 0/221, Loss: 1.6396\n",
            "Epoch 2/5, Batch 10/221, Loss: 0.4216\n",
            "Epoch 2/5, Batch 20/221, Loss: 0.6132\n",
            "Epoch 2/5, Batch 30/221, Loss: 0.3811\n",
            "Epoch 2/5, Batch 40/221, Loss: 0.3825\n",
            "Epoch 2/5, Batch 50/221, Loss: 0.3256\n",
            "Epoch 2/5, Batch 60/221, Loss: 0.2645\n",
            "Epoch 2/5, Batch 70/221, Loss: 0.2751\n",
            "Epoch 2/5, Batch 80/221, Loss: 0.5917\n",
            "Epoch 2/5, Batch 90/221, Loss: 0.1920\n",
            "Epoch 2/5, Batch 100/221, Loss: 0.2598\n",
            "Epoch 2/5, Batch 110/221, Loss: 2.1915\n",
            "Epoch 2/5, Batch 120/221, Loss: 0.3732\n",
            "Epoch 2/5, Batch 130/221, Loss: 0.5894\n",
            "Epoch 2/5, Batch 140/221, Loss: 0.2740\n",
            "Epoch 2/5, Batch 150/221, Loss: 0.4706\n",
            "Epoch 2/5, Batch 160/221, Loss: 0.3745\n",
            "Epoch 2/5, Batch 170/221, Loss: 0.2538\n",
            "Epoch 2/5, Batch 180/221, Loss: 0.2247\n",
            "Epoch 2/5, Batch 190/221, Loss: 0.2194\n",
            "Epoch 2/5, Batch 200/221, Loss: 0.2984\n",
            "Epoch 2/5, Batch 210/221, Loss: 0.4019\n",
            "Epoch 2/5, Batch 220/221, Loss: 0.1270\n",
            "Epoch 2/5, Average Loss: 0.4519\n",
            "Epoch 3/5, Batch 0/221, Loss: 0.2209\n",
            "Epoch 3/5, Batch 10/221, Loss: 0.2295\n",
            "Epoch 3/5, Batch 20/221, Loss: 0.2940\n",
            "Epoch 3/5, Batch 30/221, Loss: 0.3692\n",
            "Epoch 3/5, Batch 40/221, Loss: 0.1001\n",
            "Epoch 3/5, Batch 50/221, Loss: 0.6757\n",
            "Epoch 3/5, Batch 60/221, Loss: 0.1960\n",
            "Epoch 3/5, Batch 70/221, Loss: 0.2810\n",
            "Epoch 3/5, Batch 80/221, Loss: 0.1918\n",
            "Epoch 3/5, Batch 90/221, Loss: 0.2259\n",
            "Epoch 3/5, Batch 100/221, Loss: 0.2325\n",
            "Epoch 3/5, Batch 110/221, Loss: 0.1086\n",
            "Epoch 3/5, Batch 120/221, Loss: 0.1491\n",
            "Epoch 3/5, Batch 130/221, Loss: 0.1572\n",
            "Epoch 3/5, Batch 140/221, Loss: 0.1014\n",
            "Epoch 3/5, Batch 150/221, Loss: 0.1062\n",
            "Epoch 3/5, Batch 160/221, Loss: 0.1545\n",
            "Epoch 3/5, Batch 170/221, Loss: 0.1518\n",
            "Epoch 3/5, Batch 180/221, Loss: 0.1353\n",
            "Epoch 3/5, Batch 190/221, Loss: 0.1324\n",
            "Epoch 3/5, Batch 200/221, Loss: 0.2569\n",
            "Epoch 3/5, Batch 210/221, Loss: 0.0850\n",
            "Epoch 3/5, Batch 220/221, Loss: 0.1148\n",
            "Epoch 3/5, Average Loss: 0.2012\n",
            "Epoch 4/5, Batch 0/221, Loss: 0.4159\n",
            "Epoch 4/5, Batch 10/221, Loss: 0.0501\n",
            "Epoch 4/5, Batch 20/221, Loss: 0.1252\n",
            "Epoch 4/5, Batch 30/221, Loss: 0.0880\n",
            "Epoch 4/5, Batch 40/221, Loss: 0.0785\n",
            "Epoch 4/5, Batch 50/221, Loss: 0.1102\n",
            "Epoch 4/5, Batch 60/221, Loss: 0.0795\n",
            "Epoch 4/5, Batch 70/221, Loss: 0.1000\n",
            "Epoch 4/5, Batch 80/221, Loss: 0.1035\n",
            "Epoch 4/5, Batch 90/221, Loss: 0.2167\n",
            "Epoch 4/5, Batch 100/221, Loss: 0.0693\n",
            "Epoch 4/5, Batch 110/221, Loss: 0.1497\n",
            "Epoch 4/5, Batch 120/221, Loss: 0.0798\n",
            "Epoch 4/5, Batch 130/221, Loss: 0.1149\n",
            "Epoch 4/5, Batch 140/221, Loss: 0.0558\n",
            "Epoch 4/5, Batch 150/221, Loss: 0.1484\n",
            "Epoch 4/5, Batch 160/221, Loss: 0.1679\n",
            "Epoch 4/5, Batch 170/221, Loss: 0.1699\n",
            "Epoch 4/5, Batch 180/221, Loss: 0.0342\n",
            "Epoch 4/5, Batch 190/221, Loss: 0.1053\n",
            "Epoch 4/5, Batch 200/221, Loss: 0.1777\n",
            "Epoch 4/5, Batch 210/221, Loss: 0.0619\n",
            "Epoch 4/5, Batch 220/221, Loss: 0.0695\n",
            "Epoch 4/5, Average Loss: 0.1109\n",
            "Epoch 5/5, Batch 0/221, Loss: 0.1499\n",
            "Epoch 5/5, Batch 10/221, Loss: 0.1323\n",
            "Epoch 5/5, Batch 20/221, Loss: 0.0700\n",
            "Epoch 5/5, Batch 30/221, Loss: 0.0316\n",
            "Epoch 5/5, Batch 40/221, Loss: 0.0832\n",
            "Epoch 5/5, Batch 50/221, Loss: 0.0613\n",
            "Epoch 5/5, Batch 60/221, Loss: 0.0293\n",
            "Epoch 5/5, Batch 70/221, Loss: 0.0447\n",
            "Epoch 5/5, Batch 80/221, Loss: 0.0291\n",
            "Epoch 5/5, Batch 90/221, Loss: 0.0426\n",
            "Epoch 5/5, Batch 100/221, Loss: 0.0374\n",
            "Epoch 5/5, Batch 110/221, Loss: 0.0180\n",
            "Epoch 5/5, Batch 120/221, Loss: 0.1042\n",
            "Epoch 5/5, Batch 130/221, Loss: 0.0271\n",
            "Epoch 5/5, Batch 140/221, Loss: 0.0159\n",
            "Epoch 5/5, Batch 150/221, Loss: 0.0782\n",
            "Epoch 5/5, Batch 160/221, Loss: 0.0428\n",
            "Epoch 5/5, Batch 170/221, Loss: 0.0610\n",
            "Epoch 5/5, Batch 180/221, Loss: 0.0978\n",
            "Epoch 5/5, Batch 190/221, Loss: 0.0481\n",
            "Epoch 5/5, Batch 200/221, Loss: 0.0324\n",
            "Epoch 5/5, Batch 210/221, Loss: 0.0180\n",
            "Epoch 5/5, Batch 220/221, Loss: 0.1477\n",
            "Epoch 5/5, Average Loss: 0.0592\n",
            "Model saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "class BidirectionalLSTMQAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(BidirectionalLSTMQAModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.decoder = nn.LSTM(embedding_dim, hidden_dim * 2, batch_first=True)  # *2 because bidirectional\n",
        "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
        "\n",
        "    def forward(self, src, src_lengths, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        max_len = trg.shape[1]\n",
        "        vocab_size = self.fc.out_features\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, max_len, vocab_size).to(src.device)\n",
        "\n",
        "        # Embed the source sequence\n",
        "        embedded_src = self.embedding(src)\n",
        "\n",
        "        # Pack the embedded sequence\n",
        "        packed_src = rnn_utils.pack_padded_sequence(embedded_src, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Encode the source sequence\n",
        "        packed_outputs, (hidden, cell) = self.encoder(packed_src)\n",
        "\n",
        "        # Unpack the sequence\n",
        "        encoder_outputs, _ = rnn_utils.pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "\n",
        "        # Concatenate forward and backward hidden states\n",
        "        hidden = torch.cat((hidden[0], hidden[1]), dim=1).unsqueeze(0)\n",
        "        cell = torch.cat((cell[0], cell[1]), dim=1).unsqueeze(0)\n",
        "\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            # Embed the input\n",
        "            embedded_input = self.embedding(input).unsqueeze(1)\n",
        "\n",
        "            # Pass through the decoder\n",
        "            output, (hidden, cell) = self.decoder(embedded_input, (hidden, cell))\n",
        "\n",
        "            # Get the prediction\n",
        "            prediction = self.fc(output.squeeze(1))\n",
        "\n",
        "            # Store the prediction\n",
        "            outputs[:, t] = prediction\n",
        "\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            top1 = prediction.argmax(1)\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Function to get lengths of non-padded sequence\n",
        "def get_lengths(tensor):\n",
        "    return (tensor != 0).sum(dim=1)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = 1000\n",
        "embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "batch_size = 32\n",
        "num_epochs = 5\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "train_questions = train_questions.to(device)\n",
        "train_answers = train_answers.to(device)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(train_questions, train_answers)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = BidirectionalLSTMQAModel(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (batch_questions, batch_answers) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get lengths of non-padded sequence\n",
        "        src_lengths = get_lengths(batch_questions)\n",
        "\n",
        "        # Forward pass with teacher forcing\n",
        "        output = model(batch_questions, src_lengths, batch_answers, teacher_forcing_ratio=0.5)\n",
        "\n",
        "\n",
        "        output = output[:, 1:].reshape(-1, vocab_size)\n",
        "        target = batch_answers[:, 1:].reshape(-1)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"bidirectional_lstm_qa_model_packed_sequence.pth\")\n",
        "print(\"Model saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Fa250sOaTA",
        "outputId": "2a1e6411-891e-4a49-bc8c-4437f1a6d7c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test BLEU Score: 0.5076\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "\n",
        "test_questions = test_questions.to(device)\n",
        "test_answers = test_answers.to(device)\n",
        "\n",
        "# Create DataLoader for test data\n",
        "test_dataset = TensorDataset(test_questions, test_answers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "def evaluate_bleu(model, loader, vocab_dict):\n",
        "    model.eval()\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_questions, batch_answers in loader:\n",
        "            # Get lengths of non-padded sequence\n",
        "            src_lengths = get_lengths(batch_questions)\n",
        "\n",
        "            # Use batch_questions as both source and target for inference\n",
        "            output = model(batch_questions, src_lengths, batch_questions, teacher_forcing_ratio=0)\n",
        "            output = output.argmax(dim=2)\n",
        "\n",
        "            for i in range(output.size(0)):\n",
        "                hypothesis = indices_to_words(output[i].tolist())\n",
        "                hypothesis = [word for word in hypothesis if word not in ['<pad>', '<eos>', '<unk>']]\n",
        "                reference = indices_to_words(batch_answers[i].tolist())\n",
        "                reference = [word for word in reference if word not in ['<pad>', '<eos>', '<unk>']]\n",
        "\n",
        "                hypotheses.append(hypothesis)\n",
        "                references.append([reference])\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = corpus_bleu(references, hypotheses)\n",
        "    return bleu_score\n",
        "\n",
        "# Usage\n",
        "test_bleu_score = evaluate_bleu(model, test_loader, vocab_dict)\n",
        "print(f\"Test BLEU Score: {test_bleu_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqibgHU6PxEf"
      },
      "source": [
        "# 5. Attention Based LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "9thSI-Xdzjw9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttentionLSTMQAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(AttentionLSTMQAModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.decoder = nn.LSTMCell(embedding_dim + hidden_dim, hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "        # Adjust dimensions for key, value, and query\n",
        "        self.key_layer = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.value_layer = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.query_layer = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.attention = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        batch_size = src.shape[0]\n",
        "        max_len = trg.shape[1]\n",
        "        vocab_size = self.fc.out_features\n",
        "\n",
        "        embedded_src = self.embedding(src)\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embedded_src)\n",
        "\n",
        "        # Combine bidirectional states\n",
        "        hidden = hidden.sum(dim=0)\n",
        "        cell = cell.sum(dim=0)\n",
        "\n",
        "        keys = self.key_layer(encoder_outputs)\n",
        "        values = self.value_layer(encoder_outputs)\n",
        "\n",
        "        outputs = torch.zeros(batch_size, max_len, vocab_size).to(src.device)\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            query = self.query_layer(hidden)\n",
        "\n",
        "            attention_weights = self.calculate_attention(query, keys)\n",
        "            context = torch.bmm(attention_weights.unsqueeze(1), values).squeeze(1)\n",
        "\n",
        "            rnn_input = torch.cat((self.embedding(input), context), dim=1)\n",
        "\n",
        "            hidden, cell = self.decoder(rnn_input, (hidden, cell))\n",
        "            output = self.fc(hidden)\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            input = output.argmax(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def calculate_attention(self, query, keys):\n",
        "        energy = self.attention(torch.tanh(query.unsqueeze(1) + keys)).squeeze(2)\n",
        "        return F.softmax(energy, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMPOfXKkzl-x",
        "outputId": "f6c085f6-6fa6-48dd-ef63-d6f96c650c5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Average Loss: 3.6485, BLEU Score: 0.0030\n",
            "New best model saved with BLEU score: 0.0030\n",
            "Epoch 2/20, Average Loss: 3.0224, BLEU Score: 0.0175\n",
            "New best model saved with BLEU score: 0.0175\n",
            "Epoch 3/20, Average Loss: 2.6311, BLEU Score: 0.0291\n",
            "New best model saved with BLEU score: 0.0291\n",
            "Epoch 4/20, Average Loss: 2.3012, BLEU Score: 0.0489\n",
            "New best model saved with BLEU score: 0.0489\n",
            "Epoch 5/20, Average Loss: 2.0573, BLEU Score: 0.0566\n",
            "New best model saved with BLEU score: 0.0566\n",
            "Epoch 6/20, Average Loss: 1.8723, BLEU Score: 0.0959\n",
            "New best model saved with BLEU score: 0.0959\n",
            "Epoch 7/20, Average Loss: 1.6777, BLEU Score: 0.1206\n",
            "New best model saved with BLEU score: 0.1206\n",
            "Epoch 8/20, Average Loss: 1.5316, BLEU Score: 0.1799\n",
            "New best model saved with BLEU score: 0.1799\n",
            "Epoch 9/20, Average Loss: 1.4130, BLEU Score: 0.2081\n",
            "New best model saved with BLEU score: 0.2081\n",
            "Epoch 10/20, Average Loss: 1.3194, BLEU Score: 0.2649\n",
            "New best model saved with BLEU score: 0.2649\n",
            "Epoch 11/20, Average Loss: 1.2392, BLEU Score: 0.3011\n",
            "New best model saved with BLEU score: 0.3011\n",
            "Epoch 12/20, Average Loss: 1.1702, BLEU Score: 0.3065\n",
            "New best model saved with BLEU score: 0.3065\n",
            "Epoch 13/20, Average Loss: 1.1151, BLEU Score: 0.3372\n",
            "New best model saved with BLEU score: 0.3372\n",
            "Epoch 14/20, Average Loss: 1.0643, BLEU Score: 0.3641\n",
            "New best model saved with BLEU score: 0.3641\n",
            "Epoch 15/20, Average Loss: 1.0071, BLEU Score: 0.3913\n",
            "New best model saved with BLEU score: 0.3913\n",
            "Epoch 16/20, Average Loss: 0.9521, BLEU Score: 0.4195\n",
            "New best model saved with BLEU score: 0.4195\n",
            "Epoch 17/20, Average Loss: 0.8935, BLEU Score: 0.5016\n",
            "New best model saved with BLEU score: 0.5016\n",
            "Epoch 18/20, Average Loss: 0.7578, BLEU Score: 0.6649\n",
            "New best model saved with BLEU score: 0.6649\n",
            "Epoch 19/20, Average Loss: 0.3486, BLEU Score: 0.7901\n",
            "New best model saved with BLEU score: 0.7901\n",
            "Epoch 20/20, Average Loss: 0.0473, BLEU Score: 0.8030\n",
            "New best model saved with BLEU score: 0.8030\n",
            "Training completed.\n",
            "Best BLEU Score: 0.8030\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "import random\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "smoother = SmoothingFunction().method1\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = 1000\n",
        "embedding_dim = 512\n",
        "hidden_dim = 1024\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "learning_rate = 0.01\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_questions = train_questions.to(device)\n",
        "train_answers = train_answers.to(device)\n",
        "test_questions = test_questions.to(device)\n",
        "test_answers = test_answers.to(device)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(train_questions, train_answers)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataset = TensorDataset(test_questions, test_answers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = AttentionLSTMQAModel(vocab_size, embedding_dim, hidden_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def indices_to_words(indices, vocab_dict):\n",
        "    return [vocab_dict.get(idx, \"<unk>\") for idx in indices]\n",
        "\n",
        "def evaluate_bleu(model, loader, vocab_dict):\n",
        "    model.eval()\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_questions, batch_answers in loader:\n",
        "            batch_questions = batch_questions.to(device)\n",
        "            batch_answers = batch_answers.to(device)\n",
        "\n",
        "            output = model(batch_questions, batch_answers)\n",
        "            output = output.argmax(dim=2)  # Convert model outputs to token indices\n",
        "\n",
        "            for i in range(output.size(0)):\n",
        "                hypothesis = indices_to_words(output[i].tolist(), vocab_dict)\n",
        "                hypothesis = [word for word in hypothesis if word not in ['<pad>', '<eos>', '<unk>']]\n",
        "                reference = indices_to_words(batch_answers[i].tolist(), vocab_dict)\n",
        "                reference = [word for word in reference if word not in ['<pad>', '<eos>', '<unk>']]\n",
        "\n",
        "                hypotheses.append(hypothesis)\n",
        "                references.append([reference])\n",
        "\n",
        "    # Calculate BLEU score\n",
        "\n",
        "    bleu_score = corpus_bleu(references,hypotheses, smoothing_function=smoother)\n",
        "    return bleu_score\n",
        "\n",
        "# Training loop\n",
        "best_bleu_score = 0\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_questions, batch_answers in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(batch_questions, batch_answers)\n",
        "\n",
        "        output = output[:, 1:].reshape(-1, vocab_size)\n",
        "        target = batch_answers[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Evaluate BLEU score\n",
        "    bleu_score = evaluate_bleu(model, test_loader, vocab_dict)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}, BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "    # Save the best model\n",
        "    if bleu_score > best_bleu_score:\n",
        "        best_bleu_score = bleu_score\n",
        "        torch.save(model.state_dict(), \"best_attention_lstm_qa_model.pth\")\n",
        "        print(f\"New best model saved with BLEU score: {best_bleu_score:.4f}\")\n",
        "\n",
        "print(\"Training completed.\")\n",
        "print(f\"Best BLEU Score: {best_bleu_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py5DoCU22F6o",
        "outputId": "431e9741-05a7-4281-c448-fa0cd266b4ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1:\n",
            "Question: ▁ ▁S hould ▁I ▁call ▁my ▁dad ? ▁\n",
            "True Answer: ▁ ▁I ▁wouldn ’ t ▁know ▁how ▁to ▁ad vise ▁about ▁this . ▁\n",
            "Predicted Answer: ▁I ▁wouldn ’ t ▁know ▁how ▁to ▁ad vise ▁about ▁this . ▁\n",
            "BLEU Score: 0.9260\n",
            "\n",
            "Example 2:\n",
            "Question: ▁ ▁Do ▁I ▁look ▁attractive ▁to ▁you ▁today ? ▁\n",
            "True Answer: ▁ ▁H onestly , ▁I ▁can ' t ▁tell ▁one ▁way ▁or ▁the ▁other . ▁\n",
            "Predicted Answer: ▁H onestly , ▁I ▁can ' t ▁tell ▁one ▁way ▁or ▁the ▁other . ▁\n",
            "BLEU Score: 0.9355\n",
            "\n",
            "Example 3:\n",
            "Question: ▁ ▁How ' s ▁your ▁S unday ▁\n",
            "True Answer: ▁ ▁Good , ▁thanks . ▁\n",
            "Predicted Answer: ▁Good , ▁thanks . ▁\n",
            "BLEU Score: 0.8187\n",
            "\n",
            "Example 4:\n",
            "Question: ▁ ▁Well ▁are n ' t ▁you ▁feeling ▁happy ? ▁\n",
            "True Answer: ▁ ▁I ' m ▁quit e ▁happy , ▁thank ▁you . ▁\n",
            "Predicted Answer: ▁I ' m ▁quit e ▁happy , ▁thank ▁you . ▁\n",
            "BLEU Score: 0.9131\n",
            "\n",
            "Example 5:\n",
            "Question: ▁ ▁What ▁team ▁mad e ▁you ? ▁\n",
            "True Answer: ▁ ▁People ▁created ▁me . ▁\n",
            "Predicted Answer: ▁People ▁created ▁me . ▁\n",
            "BLEU Score: 0.8187\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "smoother = SmoothingFunction().method1\n",
        "\n",
        "def generate_answer(model, question, vocab_dict):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        question = question.unsqueeze(0).to(device)\n",
        "        max_length = 50\n",
        "\n",
        "        sos_index = next(i for i, word in vocab_dict.items() if word == \"<sos>\")\n",
        "        target = torch.tensor([[sos_index]]).to(device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            output = model(question, target)\n",
        "            next_token = output.argmax(dim=2)[:, -1]\n",
        "            target = torch.cat([target, next_token.unsqueeze(1)], dim=1)\n",
        "\n",
        "            eos_index = next(i for i, word in vocab_dict.items() if word == \"<eos>\")\n",
        "            if next_token.item() == eos_index:\n",
        "                break\n",
        "\n",
        "    return target.squeeze().tolist()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(\"best_attention_lstm_qa_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "sample_indices = random.sample(range(len(test_dataset)), 5)\n",
        "\n",
        "\n",
        "individual_bleu_scores = []\n",
        "\n",
        "for i, sample_index in enumerate(sample_indices, 1):\n",
        "    question, true_answer = test_dataset[sample_index]\n",
        "\n",
        "    # Generate predicted answer\n",
        "    predicted_answer_indices = generate_answer(model, question, vocab_dict)\n",
        "\n",
        "\n",
        "    question_words = [vocab_dict.get(idx.item(), \"<unk>\") for idx in question if idx.item() != 0]\n",
        "    true_answer_words = [vocab_dict.get(idx.item(), \"<unk>\") for idx in true_answer if idx.item() != 0]\n",
        "    predicted_answer_words = [vocab_dict.get(idx, \"<unk>\") for idx in predicted_answer_indices if idx != 0]\n",
        "\n",
        "    # Remove special tokens\n",
        "    special_tokens = {'<pad>', '<sos>', '<eos>', '<unk>'}\n",
        "    question_words = [word for word in question_words if word not in special_tokens]\n",
        "    true_answer_words = [word for word in true_answer_words if word not in special_tokens]\n",
        "    predicted_answer_words = [word for word in predicted_answer_words if word not in special_tokens]\n",
        "\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(f\"Question: {' '.join(question_words)}\")\n",
        "    print(f\"True Answer: {' '.join(true_answer_words)}\")\n",
        "    print(f\"Predicted Answer: {' '.join(predicted_answer_words)}\")\n",
        "\n",
        "\n",
        "    bleu_score = sentence_bleu([true_answer_words], predicted_answer_words, smoothing_function=smoother)\n",
        "    individual_bleu_scores.append(bleu_score)\n",
        "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "20bf69066c0dd38d51965b69d5e1b6e387082e3198ba56e97997ac55f4e50ad0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
